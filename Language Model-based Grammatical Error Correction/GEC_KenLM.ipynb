{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GEC-KenLM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "971174884b444e8cb5b5fd3af54bc1fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_27d35393d9c2449c9355dc8991b7eaf2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_61d2ba04df39455499d0c68effbc690a",
              "IPY_MODEL_4a2a34e2296a4ef19c28087142cd4e9a"
            ]
          }
        },
        "27d35393d9c2449c9355dc8991b7eaf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61d2ba04df39455499d0c68effbc690a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aa4c54f9016f40fda141bc88ca736fa5",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1313,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1313,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae7e7713ae1f456d9af47d6edbcba30c"
          }
        },
        "4a2a34e2296a4ef19c28087142cd4e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2c3cbfd6944b423d9c4c54c7a8ff6f73",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1313/1313 [00:19&lt;00:00, 66.90it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6289b6465bd44f428f51ff2b4784a162"
          }
        },
        "aa4c54f9016f40fda141bc88ca736fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae7e7713ae1f456d9af47d6edbcba30c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c3cbfd6944b423d9c4c54c7a8ff6f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6289b6465bd44f428f51ff2b4784a162": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMaxpmiLKk2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "1398b489-2354-4999-a82b-f7b173a0eb1f"
      },
      "source": [
        "#test set\n",
        "!gdown --id 1CHnRNybDYbq9xTZNCxf22PIQIyBe0Yfz"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CHnRNybDYbq9xTZNCxf22PIQIyBe0Yfz\n",
            "To: /content/kenlm/build/gec-test-set.txt\n",
            "\r  0% 0.00/162k [00:00<?, ?B/s]\r100% 162k/162k [00:00<00:00, 69.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1MzOIXKj1Te",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "332b0875-0e8e-4a61-ebd2-b6e144510b33"
      },
      "source": [
        "!pip3 install pyinflect\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyinflect in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr1VvJzHoM6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "  \n",
        "lemmatizer = WordNetLemmatizer() "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxgk_5O6oTCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pos_tag(sentence):\n",
        "   tokens = word_tokenize(sentence)\n",
        "   return nltk.pos_tag(tokens)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_1wlHhJY_XG",
        "colab_type": "text"
      },
      "source": [
        "# **Function for generating candidate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tumkPpImjpjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyinflect import getAllInflections, getInflection\n",
        "from nltk import word_tokenize\n",
        "import spacy\n",
        "import re\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDsm_UXcOAcd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "b5f092a3-0f93-45a3-cbd2-3d496f71b426"
      },
      "source": [
        "!git clone https://github.com/nozomiyamada/contest2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'contest2'...\n",
            "remote: Enumerating objects: 169, done.\u001b[K\n",
            "remote: Counting objects: 100% (169/169), done.\u001b[K\n",
            "remote: Compressing objects: 100% (152/152), done.\u001b[K\n",
            "remote: Total 169 (delta 47), reused 94 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (169/169), 6.59 MiB | 5.53 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVB14_3aOy2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "#noun.csv\n",
        "noun_dict2 = dict()\n",
        "noun_list = []\n",
        "with open('/content/contest2/noun.json') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "  for row in csv_reader:\n",
        "    for string in row:\n",
        "      noun_list.append(string.strip(' ').strip(\"]\").strip(\"[\").strip(' \" '))\n",
        "for i in range(len(noun_list)-1):\n",
        "  noun_dict2[noun_list[i]] = noun_list[i : i+2]\n",
        "\n",
        "#verb.csv\n",
        "verb_dict2 = dict()\n",
        "with open('/content/contest2/verbs-dictionaries.csv') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file,delimiter =',')\n",
        "  for row in csv_reader:\n",
        "    for verbs in row:\n",
        "      verbs_list = verbs.split('\\t')\n",
        "      verb_dict2[verbs_list[0]] = verbs_list[:]\n",
        "\n",
        "#prep.json\n",
        "prep_list = []\n",
        "with open('/content/contest2/prep.json') as f:\n",
        "  data = json.load(f)\n",
        "  #prep_list.append(\"\")\n",
        "  for prep in data:\n",
        "    prep_list.append(prep)\n",
        "  "
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5yBM1RjuwEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modal = [\"will\",\"would\",\"must\",\"shall\",\"should\",\"may\", \"might\",\"can\",\"could\" ]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYVqzxlmIReq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#find candidates\n",
        "\n",
        "def pos_tag(sentence):\n",
        "   tokens = word_tokenize(sentence)\n",
        "   return nltk.pos_tag(tokens)\n",
        "\n",
        "def lemma(word):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(word)\n",
        "  lemma_word = [token.lemma_ for token in doc]\n",
        "  return lemma_word\n",
        "\n",
        "def verb_form_candidate(word):\n",
        "  infection_dict = getAllInflections(word , 'V')\n",
        "  if word in verb_dict2:\n",
        "    get_infection = verb_dict2[word]\n",
        "  else:\n",
        "    get_infection = [tup[0] for tup in infection_dict.values()]\n",
        "    get_infection.append(word)\n",
        "  return get_infection\n",
        "\n",
        "def be_verb():\n",
        "  return ['be', 'is', 'am', 'are','was', 'were', 'been','being']\n",
        "\n",
        "def article_candidate():\n",
        "  return ['an','a','the',\"some\",'any','this','that','those','these', '']#,\"each\",'every','all','either','neither']\n",
        "\n",
        "def  noun_candidate(word):\n",
        "  noun_dict1 = getAllInflections(word , 'N')\n",
        "  if len(word) > 1 and word in noun_dict2:\n",
        "    get_forms = noun_dict2[word]\n",
        "  else:\n",
        "    get_forms = [tup[0] for tup in noun_dict1.values()]\n",
        "    if len(get_forms) == 0:\n",
        "      get_forms.append(word)\n",
        "      get_forms.append(word + 's')\n",
        "      if word.endswith('y') and len(word) > 1 and word[-2] not in ['a','e','i','o','u']:\n",
        "        get_forms.append(word[ : -1] + 'ies')\n",
        "  return get_forms\n",
        "\n",
        "def prep_candidate():\n",
        "   return prep_list\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdlHjc3Lx7Ep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a6c3193-d6d7-423f-80f0-37005cba9a9c"
      },
      "source": [
        "prep_candidate()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aboard',\n",
              " 'about',\n",
              " 'above',\n",
              " 'absent',\n",
              " 'across',\n",
              " 'after',\n",
              " 'against',\n",
              " 'along',\n",
              " 'alongside',\n",
              " 'amid',\n",
              " 'amidst',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'around',\n",
              " 'as',\n",
              " 'astride',\n",
              " 'at',\n",
              " 'atop',\n",
              " 'before',\n",
              " 'afore',\n",
              " 'behind',\n",
              " 'below',\n",
              " 'beneath',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'by',\n",
              " 'circa',\n",
              " 'despite',\n",
              " 'down',\n",
              " 'during',\n",
              " 'except',\n",
              " 'for',\n",
              " 'from',\n",
              " 'in',\n",
              " 'inside',\n",
              " 'into',\n",
              " 'less',\n",
              " 'like',\n",
              " 'minus',\n",
              " 'near',\n",
              " 'nearer',\n",
              " 'nearest',\n",
              " 'notwithstanding',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'onto',\n",
              " 'opposite',\n",
              " 'outside',\n",
              " 'over',\n",
              " 'past',\n",
              " 'per',\n",
              " 'save',\n",
              " 'since',\n",
              " 'through',\n",
              " 'throughout',\n",
              " 'to',\n",
              " 'toward',\n",
              " 'towards',\n",
              " 'under',\n",
              " 'underneath',\n",
              " 'until',\n",
              " 'up',\n",
              " 'upon',\n",
              " 'upside',\n",
              " 'versus',\n",
              " 'via',\n",
              " 'with',\n",
              " 'within',\n",
              " 'without',\n",
              " 'worth',\n",
              " 'according to',\n",
              " 'adjacent to',\n",
              " 'ahead of',\n",
              " 'apart from',\n",
              " 'as of',\n",
              " 'as per',\n",
              " 'as regards',\n",
              " 'aside from',\n",
              " 'astern of',\n",
              " 'back to',\n",
              " 'because of',\n",
              " 'close to',\n",
              " 'due to',\n",
              " 'except for',\n",
              " 'far from',\n",
              " 'inside of',\n",
              " 'instead of',\n",
              " 'left of',\n",
              " 'near to',\n",
              " 'next to',\n",
              " 'opposite of',\n",
              " 'opposite to',\n",
              " 'out from',\n",
              " 'out of',\n",
              " 'outside of',\n",
              " 'owing to',\n",
              " 'prior to',\n",
              " 'pursuant to',\n",
              " 'rather than',\n",
              " 'regardless of',\n",
              " 'right of',\n",
              " 'subsequent to',\n",
              " 'such as',\n",
              " 'thanks to',\n",
              " 'up to',\n",
              " 'as far as',\n",
              " 'as opposed to',\n",
              " 'as soon as',\n",
              " 'as well as',\n",
              " 'at the behest of',\n",
              " 'by means of',\n",
              " 'by virtue of',\n",
              " 'for the sake of',\n",
              " 'in accordance with',\n",
              " 'in addition to',\n",
              " 'in case of',\n",
              " 'in front of',\n",
              " 'in lieu of',\n",
              " 'in place of',\n",
              " 'in point of',\n",
              " 'in spite of',\n",
              " 'on account of',\n",
              " 'on behalf of',\n",
              " 'on top of',\n",
              " 'with regard to',\n",
              " 'with respect to',\n",
              " 'with a view to']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfrwk1hQ7oVG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d52b8d95-c77f-48fb-a836-c85495ce50c7"
      },
      "source": [
        "noun_candidate(\"party\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['party', 'parties']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr2T2e_Q3tsi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67680148-45e0-4f46-d3e0-94cbcfb9349d"
      },
      "source": [
        "prep_candidate()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aboard',\n",
              " 'about',\n",
              " 'above',\n",
              " 'absent',\n",
              " 'across',\n",
              " 'after',\n",
              " 'against',\n",
              " 'along',\n",
              " 'alongside',\n",
              " 'amid',\n",
              " 'amidst',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'around',\n",
              " 'as',\n",
              " 'astride',\n",
              " 'at',\n",
              " 'atop',\n",
              " 'before',\n",
              " 'afore',\n",
              " 'behind',\n",
              " 'below',\n",
              " 'beneath',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'by',\n",
              " 'circa',\n",
              " 'despite',\n",
              " 'down',\n",
              " 'during',\n",
              " 'except',\n",
              " 'for',\n",
              " 'from',\n",
              " 'in',\n",
              " 'inside',\n",
              " 'into',\n",
              " 'less',\n",
              " 'like',\n",
              " 'minus',\n",
              " 'near',\n",
              " 'nearer',\n",
              " 'nearest',\n",
              " 'notwithstanding',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'onto',\n",
              " 'opposite',\n",
              " 'outside',\n",
              " 'over',\n",
              " 'past',\n",
              " 'per',\n",
              " 'save',\n",
              " 'since',\n",
              " 'through',\n",
              " 'throughout',\n",
              " 'to',\n",
              " 'toward',\n",
              " 'towards',\n",
              " 'under',\n",
              " 'underneath',\n",
              " 'until',\n",
              " 'up',\n",
              " 'upon',\n",
              " 'upside',\n",
              " 'versus',\n",
              " 'via',\n",
              " 'with',\n",
              " 'within',\n",
              " 'without',\n",
              " 'worth',\n",
              " 'according to',\n",
              " 'adjacent to',\n",
              " 'ahead of',\n",
              " 'apart from',\n",
              " 'as of',\n",
              " 'as per',\n",
              " 'as regards',\n",
              " 'aside from',\n",
              " 'astern of',\n",
              " 'back to',\n",
              " 'because of',\n",
              " 'close to',\n",
              " 'due to',\n",
              " 'except for',\n",
              " 'far from',\n",
              " 'inside of',\n",
              " 'instead of',\n",
              " 'left of',\n",
              " 'near to',\n",
              " 'next to',\n",
              " 'opposite of',\n",
              " 'opposite to',\n",
              " 'out from',\n",
              " 'out of',\n",
              " 'outside of',\n",
              " 'owing to',\n",
              " 'prior to',\n",
              " 'pursuant to',\n",
              " 'rather than',\n",
              " 'regardless of',\n",
              " 'right of',\n",
              " 'subsequent to',\n",
              " 'such as',\n",
              " 'thanks to',\n",
              " 'up to',\n",
              " 'as far as',\n",
              " 'as opposed to',\n",
              " 'as soon as',\n",
              " 'as well as',\n",
              " 'at the behest of',\n",
              " 'by means of',\n",
              " 'by virtue of',\n",
              " 'for the sake of',\n",
              " 'in accordance with',\n",
              " 'in addition to',\n",
              " 'in case of',\n",
              " 'in front of',\n",
              " 'in lieu of',\n",
              " 'in place of',\n",
              " 'in point of',\n",
              " 'in spite of',\n",
              " 'on account of',\n",
              " 'on behalf of',\n",
              " 'on top of',\n",
              " 'with regard to',\n",
              " 'with respect to',\n",
              " 'with a view to']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh5YqX92BcW2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9174c695-2abf-459a-bf34-b483a457eec1"
      },
      "source": [
        "pos_tag('I am a good boy')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('good', 'JJ'), ('boy', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcbEblnoaKsu",
        "colab_type": "text"
      },
      "source": [
        "# **Sentences**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSszgYkVxvXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare sentence\n",
        "def pos_tag(sentence):\n",
        "  pos_sentences=nltk.pos_tag(sentence.split())"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isU67DdXrPdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_tag('I go shopping')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBPRXZuadqQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5f4b37cc-8e4b-421e-d854-02db89d29375"
      },
      "source": [
        "with open('/content/gec-test-set.txt') as file:\n",
        "  data = file.read().split('\\n')\n",
        "  sentences = [s for s in data]\n",
        "sentences[100]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Genetic testing is made possible and available for one 's decision to undergo .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g0nnQqvaUp1",
        "colab_type": "text"
      },
      "source": [
        "# **LM Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdmUvZXaIr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "a2e3fca1-e9b5-4671-fb8e-e734521dd92c"
      },
      "source": [
        "#Install KenLM Library\n",
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
        "!mkdir kenlm/build"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-08 13:34:22--  https://kheafield.com/code/kenlm.tar.gz\n",
            "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
            "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 490441 (479K) [application/x-gzip]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>] 478.95K   517KB/s    in 0.9s    \n",
            "\n",
            "2020-08-08 13:34:24 (517 KB/s) - written to stdout [490441/490441]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drq07Q5yhtYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a702e8dd-74e4-4124-cb5f-f73d1e47ad1a"
      },
      "source": [
        "%cd kenlm/build\n",
        "!cmake ..\n",
        "!make -j2"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/kenlm/build\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Looking for pthread_create\n",
            "-- Looking for pthread_create - not found\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Boost version: 1.65.1\n",
            "-- Found the following Boost libraries:\n",
            "--   program_options\n",
            "--   system\n",
            "--   thread\n",
            "--   unit_test_framework\n",
            "--   chrono\n",
            "--   date_time\n",
            "--   atomic\n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.6\") \n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/include (found version \"5.2.2\") \n",
            "-- Could NOT find Eigen3 (missing: EIGEN3_INCLUDE_DIR EIGEN3_VERSION_OK) (Required is at least version \"2.91.0\")\n",
            "CMake Warning at lm/interpolate/CMakeLists.txt:65 (message):\n",
            "  Not building interpolation.  Eigen3 was not found.\n",
            "\n",
            "\n",
            "-- To install Eigen3 in your home directory, copy paste this:\n",
            "export EIGEN3_ROOT=$HOME/eigen-eigen-07105f7124f9\n",
            "(cd $HOME; wget -O - https://bitbucket.org/eigen/eigen/get/3.2.8.tar.bz2 |tar xj)\n",
            "rm CMakeCache.txt\n",
            "\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/kenlm/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_filter\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_util\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 16%] Built target kenlm_filter\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 43%] Built target kenlm_util\n",
            "\u001b[35m\u001b[1mScanning dependencies of target probing_hash_table_benchmark\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 55%] Built target probing_hash_table_benchmark\n",
            "[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target kenlm\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fragment\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target build_binary\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 75%] Built target fragment\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_benchmark\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 77%] Built target build_binary\n",
            "\u001b[35m\u001b[1mScanning dependencies of target query\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 80%] Built target query\n",
            "\u001b[35m\u001b[1mScanning dependencies of target kenlm_builder\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 87%] Built target kenlm_benchmark\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target phrase_table_vocab\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 91%] Built target phrase_table_vocab\n",
            "\u001b[35m\u001b[1mScanning dependencies of target filter\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 93%] Built target kenlm_builder\n",
            "\u001b[35m\u001b[1mScanning dependencies of target count_ngrams\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[ 96%] Built target count_ngrams\n",
            "\u001b[35m\u001b[1mScanning dependencies of target lmplz\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 98%] Built target filter\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[100%] Built target lmplz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFqfETNxh6lf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "91afbfe2-2eba-4d54-beab-626fb84efc80"
      },
      "source": [
        "#Dowload news data \n",
        "!gdown --id 1tGt5FJIZ0RPKAz4NxxhL1PvNd85AJBQu"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tGt5FJIZ0RPKAz4NxxhL1PvNd85AJBQu\n",
            "To: /content/kenlm/build/gigaword3_nyt_eng.tar.gz\n",
            "2.54GB [00:18, 138MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfcQBBm1h-Wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf gigaword3_nyt_eng.tar.gz"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdLu_0pxG6ly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "025fdd49-5342-4d73-d9e6-93806f53b250"
      },
      "source": [
        "#Dowload gutenberg\n",
        "!gdown --id 1PeMJ8Z9HBDravgGzZ1baDRnliHRf1O75"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PeMJ8Z9HBDravgGzZ1baDRnliHRf1O75\n",
            "To: /content/kenlm/build/Gutenberg.zip\n",
            "462MB [00:02, 180MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec6OIPGPHCjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile('/content/kenlm/build/Gutenberg.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciFz8HRFiCpk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "outputId": "61d84f60-d4bb-433f-eca0-c63dde389514"
      },
      "source": [
        "ls #/content/kenlm/build/kenlm/build/kenlm"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbin\u001b[0m/                      nyt_eng_199701  nyt_eng_200005  nyt_eng_200309\n",
            "CMakeCache.txt            nyt_eng_199702  nyt_eng_200006  nyt_eng_200310\n",
            "\u001b[01;34mCMakeFiles\u001b[0m/               nyt_eng_199703  nyt_eng_200007  nyt_eng_200311\n",
            "cmake_install.cmake       nyt_eng_199704  nyt_eng_200008  nyt_eng_200312\n",
            "gigaword3_nyt_eng.tar.gz  nyt_eng_199705  nyt_eng_200009  nyt_eng_200401\n",
            "\u001b[01;34mGutenberg\u001b[0m/                nyt_eng_199706  nyt_eng_200010  nyt_eng_200402\n",
            "Gutenberg.zip             nyt_eng_199707  nyt_eng_200011  nyt_eng_200403\n",
            "\u001b[01;34mlib\u001b[0m/                      nyt_eng_199708  nyt_eng_200012  nyt_eng_200404\n",
            "\u001b[01;34mlm\u001b[0m/                       nyt_eng_199709  nyt_eng_200101  nyt_eng_200405\n",
            "Makefile                  nyt_eng_199710  nyt_eng_200102  nyt_eng_200407\n",
            "nyt_eng_199407            nyt_eng_199711  nyt_eng_200103  nyt_eng_200408\n",
            "nyt_eng_199408            nyt_eng_199712  nyt_eng_200104  nyt_eng_200409\n",
            "nyt_eng_199409            nyt_eng_199801  nyt_eng_200105  nyt_eng_200410\n",
            "nyt_eng_199410            nyt_eng_199802  nyt_eng_200106  nyt_eng_200411\n",
            "nyt_eng_199411            nyt_eng_199803  nyt_eng_200107  nyt_eng_200412\n",
            "nyt_eng_199412            nyt_eng_199804  nyt_eng_200108  nyt_eng_200501\n",
            "nyt_eng_199501            nyt_eng_199805  nyt_eng_200109  nyt_eng_200502\n",
            "nyt_eng_199502            nyt_eng_199806  nyt_eng_200110  nyt_eng_200503\n",
            "nyt_eng_199503            nyt_eng_199807  nyt_eng_200111  nyt_eng_200504\n",
            "nyt_eng_199504            nyt_eng_199808  nyt_eng_200112  nyt_eng_200505\n",
            "nyt_eng_199505            nyt_eng_199809  nyt_eng_200201  nyt_eng_200506\n",
            "nyt_eng_199506            nyt_eng_199810  nyt_eng_200202  nyt_eng_200507\n",
            "nyt_eng_199507            nyt_eng_199811  nyt_eng_200203  nyt_eng_200508\n",
            "nyt_eng_199508            nyt_eng_199812  nyt_eng_200204  nyt_eng_200509\n",
            "nyt_eng_199509            nyt_eng_199901  nyt_eng_200205  nyt_eng_200510\n",
            "nyt_eng_199510            nyt_eng_199902  nyt_eng_200206  nyt_eng_200511\n",
            "nyt_eng_199511            nyt_eng_199903  nyt_eng_200207  nyt_eng_200512\n",
            "nyt_eng_199512            nyt_eng_199904  nyt_eng_200208  nyt_eng_200601\n",
            "nyt_eng_199601            nyt_eng_199905  nyt_eng_200209  nyt_eng_200602\n",
            "nyt_eng_199602            nyt_eng_199906  nyt_eng_200210  nyt_eng_200603\n",
            "nyt_eng_199603            nyt_eng_199907  nyt_eng_200211  nyt_eng_200604\n",
            "nyt_eng_199604            nyt_eng_199908  nyt_eng_200212  nyt_eng_200605\n",
            "nyt_eng_199605            nyt_eng_199909  nyt_eng_200301  nyt_eng_200606\n",
            "nyt_eng_199606            nyt_eng_199910  nyt_eng_200302  nyt_eng_200607\n",
            "nyt_eng_199607            nyt_eng_199911  nyt_eng_200303  nyt_eng_200608\n",
            "nyt_eng_199608            nyt_eng_199912  nyt_eng_200304  nyt_eng_200609\n",
            "nyt_eng_199609            nyt_eng_200001  nyt_eng_200305  nyt_eng_200610\n",
            "nyt_eng_199610            nyt_eng_200002  nyt_eng_200306  nyt_eng_200611\n",
            "nyt_eng_199611            nyt_eng_200003  nyt_eng_200307  nyt_eng_200612\n",
            "nyt_eng_199612            nyt_eng_200004  nyt_eng_200308  \u001b[01;34mutil\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrRXTngNWLmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat nyt_eng_2001* nyt_eng_2006*  > nyt_eng.txt"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2V7HKfXiIBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "f6f13d52-3046-4f2b-c299-d4107a7003ec"
      },
      "source": [
        "#Install KenLM in Python\n",
        "!pip install https://github.com/kpu/kenlm/archive/master.zip"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[K     / 2.9MB 377kB/s\n",
            "\u001b[?25hBuilding wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.0.0-cp36-cp36m-linux_x86_64.whl size=2324509 sha256=dd7e463592b2db4cb5fa97af0f3d1d6ade6279b345c171ba01b0915377840060\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gqi977p0/wheels/2d/32/73/e3093c9d11dc8abf79c156a4db1a1c5631428059d4f9ff2cba\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4ErtpzmiPc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "outputId": "a0704ec4-fb59-4aaa-be1e-7a218faacdc7"
      },
      "source": [
        "#Train LM with KenLM\n",
        "!bin/lmplz -o 6 --text nyt_eng.txt --arpa sixgram.arpa"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/kenlm/build/nyt_eng.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 3456286720 bytes == 0x5603b92e0000 @  0x7fa64b1401e7 0x5603b7ee0772 0x5603b7e74358 0x5603b7e53290 0x5603b7e3f096 0x7fa6492d9b97 0x5603b7e40ada\n",
            "tcmalloc: large alloc 18433531904 bytes == 0x56048730c000 @  0x7fa64b1401e7 0x5603b7ee0772 0x5603b7eca7aa 0x5603b7ecb1c8 0x5603b7e532ad 0x5603b7e3f096 0x7fa6492d9b97 0x5603b7e40ada\n",
            "****************************************************************************************************\n",
            "Unigram tokens 209075440 types 880985\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:10571820 2:1347963776 3:2527431936 4:4043891200 5:5897341440 6:8087782400\n",
            "tcmalloc: large alloc 8087789568 bytes == 0x5603b92e0000 @  0x7fa64b1401e7 0x5603b7ee0772 0x5603b7eca7aa 0x5603b7ecb1c8 0x5603b7e5384e 0x5603b7e3f096 0x7fa6492d9b97 0x5603b7e40ada\n",
            "tcmalloc: large alloc 2527436800 bytes == 0x5605ec3a2000 @  0x7fa64b1401e7 0x5603b7ee0772 0x5603b7eca7aa 0x5603b7ecb1c8 0x5603b7e53c3d 0x5603b7e3f096 0x7fa6492d9b97 0x5603b7e40ada\n",
            "tcmalloc: large alloc 4043898880 bytes == 0x560682dfc000 @  0x7fa64b1401e7 0x5603b7ee0772 0x5603b7eca7aa 0x5603b7ecb1c8 0x5603b7e53c3d 0x5603b7e3f096 0x7fa6492d9b97 0x5603b7e40ada\n",
            "tcmalloc: large alloc 5897347072 bytes == 0x5608d2f46000 @  0x7fa64b1401e7 0x5603b7ee0772 0x5603b7eca7aa 0x5603b7ecb1c8 0x5603b7e53c3d 0x5603b7e3f096 0x7fa6492d9b97 0x5603b7e40ada\n",
            "Statistics:\n",
            "1 880985 D1=0.680597 D2=0.997432 D3+=1.31674\n",
            "2 15721743 D1=0.722756 D2=1.08151 D3+=1.38691\n",
            "3 60179756 D1=0.80287 D2=1.14142 D3+=1.35163\n",
            "4 104114180 D1=0.874009 D2=1.24055 D3+=1.33935\n",
            "5 120994436 D1=0.924204 D2=1.37909 D3+=1.28531\n",
            "6 118342352 D1=0.806938 D2=1.6065 D3+=0.878681\n",
            "Memory estimate for binary LM:\n",
            "type       MB\n",
            "probing  8942 assuming -p 1.5\n",
            "probing 10668 assuming -r models -p 1.5\n",
            "trie     4684 without quantization\n",
            "trie     2673 assuming -q 8 -b 8 quantization \n",
            "trie     3979 assuming -a 22 array pointer compression\n",
            "trie     1968 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "tcmalloc: large alloc 2498748416 bytes == 0x5608daf46000 @  0x7fa64b1401e7 0x5603b7ee0772 0x5603b7e5e232 0x5603b7e5f1cd 0x7fa64aa73bcd 0x7fa64a84a6db 0x7fa6493d9a3f\n",
            "tcmalloc: large alloc 3387850752 bytes == 0x5608daf46000 @  0x7fa64b1401e7 0x5603b7ee0772 0x5603b7e5e232 0x5603b7e5f1cd 0x7fa64aa73bcd 0x7fa64a84a6db 0x7fa6493d9a3f\n",
            "tcmalloc: large alloc 3786956800 bytes == 0x5608daf46000 @  0x7fa64b1401e7 0x5603b7ee0772 0x5603b7e5e232 0x5603b7e5f1cd 0x7fa64aa73bcd 0x7fa64a84a6db 0x7fa6493d9a3f\n",
            "Chain sizes: 1:10571820 2:251547888 3:1203595120 4:2498740320 5:3387844208 6:3786955264\n",
            "tcmalloc: large alloc 3387850752 bytes == 0x5603b92e0000 @  0x7fa64b1401e7 0x5603b7ee0772 0x5603b7eca7aa 0x5603b7ecb1c8 0x5603b7e59cb1 0x5603b7e51c2e 0x5603b7e542e9 0x5603b7e3f096 0x7fa6492d9b97 0x5603b7e40ada\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 3786956800 bytes == 0x5604831c8000 @  0x7fa64b1401e7 0x5603b7ee0772 0x5603b7eca7aa 0x5603b7ecb1c8 0x5603b7e59cb1 0x5603b7e51c2e 0x5603b7e542e9 0x5603b7e3f096 0x7fa6492d9b97 0x5603b7e40ada\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:10571820 2:251547888 3:1203595120 4:2498740320 5:3387844208 6:3786955264\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:27684460 kB\tVmRSS:10914872 kB\tRSSMax:15104424 kB\tuser:588.228\tsys:115.19\tCPU:703.419\treal:1488.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w3w1PAGigRP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "outputId": "d6f5f056-01da-4ff9-d8ae-ac04979689c5"
      },
      "source": [
        "ls"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbin\u001b[0m/                      nyt_eng_199702  nyt_eng_200007  nyt_eng_200312\n",
            "CMakeCache.txt            nyt_eng_199703  nyt_eng_200008  nyt_eng_200401\n",
            "\u001b[01;34mCMakeFiles\u001b[0m/               nyt_eng_199704  nyt_eng_200009  nyt_eng_200402\n",
            "cmake_install.cmake       nyt_eng_199705  nyt_eng_200010  nyt_eng_200403\n",
            "gigaword3_nyt_eng.tar.gz  nyt_eng_199706  nyt_eng_200011  nyt_eng_200404\n",
            "\u001b[01;34mGutenberg\u001b[0m/                nyt_eng_199707  nyt_eng_200012  nyt_eng_200405\n",
            "Gutenberg.zip             nyt_eng_199708  nyt_eng_200101  nyt_eng_200407\n",
            "\u001b[01;34mlib\u001b[0m/                      nyt_eng_199709  nyt_eng_200102  nyt_eng_200408\n",
            "\u001b[01;34mlm\u001b[0m/                       nyt_eng_199710  nyt_eng_200103  nyt_eng_200409\n",
            "Makefile                  nyt_eng_199711  nyt_eng_200104  nyt_eng_200410\n",
            "nyt_eng_199407            nyt_eng_199712  nyt_eng_200105  nyt_eng_200411\n",
            "nyt_eng_199408            nyt_eng_199801  nyt_eng_200106  nyt_eng_200412\n",
            "nyt_eng_199409            nyt_eng_199802  nyt_eng_200107  nyt_eng_200501\n",
            "nyt_eng_199410            nyt_eng_199803  nyt_eng_200108  nyt_eng_200502\n",
            "nyt_eng_199411            nyt_eng_199804  nyt_eng_200109  nyt_eng_200503\n",
            "nyt_eng_199412            nyt_eng_199805  nyt_eng_200110  nyt_eng_200504\n",
            "nyt_eng_199501            nyt_eng_199806  nyt_eng_200111  nyt_eng_200505\n",
            "nyt_eng_199502            nyt_eng_199807  nyt_eng_200112  nyt_eng_200506\n",
            "nyt_eng_199503            nyt_eng_199808  nyt_eng_200201  nyt_eng_200507\n",
            "nyt_eng_199504            nyt_eng_199809  nyt_eng_200202  nyt_eng_200508\n",
            "nyt_eng_199505            nyt_eng_199810  nyt_eng_200203  nyt_eng_200509\n",
            "nyt_eng_199506            nyt_eng_199811  nyt_eng_200204  nyt_eng_200510\n",
            "nyt_eng_199507            nyt_eng_199812  nyt_eng_200205  nyt_eng_200511\n",
            "nyt_eng_199508            nyt_eng_199901  nyt_eng_200206  nyt_eng_200512\n",
            "nyt_eng_199509            nyt_eng_199902  nyt_eng_200207  nyt_eng_200601\n",
            "nyt_eng_199510            nyt_eng_199903  nyt_eng_200208  nyt_eng_200602\n",
            "nyt_eng_199511            nyt_eng_199904  nyt_eng_200209  nyt_eng_200603\n",
            "nyt_eng_199512            nyt_eng_199905  nyt_eng_200210  nyt_eng_200604\n",
            "nyt_eng_199601            nyt_eng_199906  nyt_eng_200211  nyt_eng_200605\n",
            "nyt_eng_199602            nyt_eng_199907  nyt_eng_200212  nyt_eng_200606\n",
            "nyt_eng_199603            nyt_eng_199908  nyt_eng_200301  nyt_eng_200607\n",
            "nyt_eng_199604            nyt_eng_199909  nyt_eng_200302  nyt_eng_200608\n",
            "nyt_eng_199605            nyt_eng_199910  nyt_eng_200303  nyt_eng_200609\n",
            "nyt_eng_199606            nyt_eng_199911  nyt_eng_200304  nyt_eng_200610\n",
            "nyt_eng_199607            nyt_eng_199912  nyt_eng_200305  nyt_eng_200611\n",
            "nyt_eng_199608            nyt_eng_200001  nyt_eng_200306  nyt_eng_200612\n",
            "nyt_eng_199609            nyt_eng_200002  nyt_eng_200307  nyt_eng.txt\n",
            "nyt_eng_199610            nyt_eng_200003  nyt_eng_200308  sixgram.arpa\n",
            "nyt_eng_199611            nyt_eng_200004  nyt_eng_200309  \u001b[01;34mutil\u001b[0m/\n",
            "nyt_eng_199612            nyt_eng_200005  nyt_eng_200310\n",
            "nyt_eng_199701            nyt_eng_200006  nyt_eng_200311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUgYZBwKilOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import kenlm\n",
        "model = kenlm.Model('sixgram.arpa')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8sGvvh9InWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, s):\n",
        "  tokens = s.split(' ')\n",
        "  log_score = 0.0\n",
        "  for i, (logprob, length, oov) in enumerate(model.full_scores(s,bos = True, eos = True)):\n",
        "    if i < len(tokens):\n",
        "     tokens[i], math.exp(logprob), oov\n",
        "    else:\n",
        "      'END', math.exp(logprob), oov\n",
        "  \n",
        "    log_score += logprob\n",
        "  return log_score"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6NuFB80imEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def print_score(model, s):\n",
        "  tokens = s.split(' ')\n",
        "  log_score = 0.0\n",
        "  for i, (logprob, length, oov) in enumerate(model.full_scores(s)):\n",
        "    if i < len(tokens):\n",
        "      print(tokens[i], math.exp(logprob), oov)\n",
        "    else:\n",
        "      print('END', math.exp(logprob), oov)\n",
        "  \n",
        "    log_score += logprob\n",
        "  return log_score"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM5UgAsQiqew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "a7277097-374d-4c33-db73-92f8c394720b"
      },
      "source": [
        "print_score(model, \"I can't hardly believe what he said.\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I 0.07113961029170372 False\n",
            "can't 6.730817696776554e-05 True\n",
            "hardly 0.01296525295734166 False\n",
            "believe 0.05890259809413126 False\n",
            "what 0.24527611272994368 False\n",
            "he 0.38032571191507625 False\n",
            "said. 0.00011563156058634109 False\n",
            "END 0.11392771131013155 False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-33.03608298301697"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfCBlYsAlKlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Edit function\n",
        "\n",
        "def edit_article(word,sentence):\n",
        "  sentence_candidates = []\n",
        "  best_candidate = sentence\n",
        "  for candidate in article_candidate():\n",
        "      new_sentence = re.sub(r'\\b' + word + r'\\b' , candidate ,sentence,count = 1)  \n",
        "      sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "  rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "  best_candidate = rank[0][0] #ถ้ามี 2 the ในประโยค?\n",
        "  return best_candidate\n",
        "\n",
        "def edit_be_verb(word,sentence):\n",
        "  sentence_candidates = []\n",
        "  for candidate in be_verb():\n",
        "    new_sentence = re.sub(r'\\b' + word + r'\\b' , candidate ,sentence,count = 1)  \n",
        "    sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "  rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "  best_candidate = rank[0][0]\n",
        "  return best_candidate\n",
        "\n",
        "def edit_verb_forms(word,sentence):\n",
        "  sentence_candidates = []\n",
        "  word =lemma(word)[0]\n",
        "  for candidate in verb_form_candidate(word):\n",
        "    new_sentence = re.sub(r'\\b' + word + r'\\b' , candidate ,sentence,count = 1)  \n",
        "    sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "  rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "  best_candidate = rank[0][0]\n",
        "  return best_candidate\n",
        "\n",
        "def edit_noun(word,sentence):\n",
        "  sentence_candidates = []\n",
        "  word =lemma(word)[0]\n",
        "  for candidate in noun_candidate(word):\n",
        "    new_sentence = re.sub(r'\\b' + word + r'\\b' , candidate ,sentence,count = 1)  \n",
        "    sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "  rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "  best_candidate = rank[0][0]\n",
        "  return best_candidate\n",
        "\n",
        "#Error\n",
        "def edit_part_of_speech(word,sentence):\n",
        "  sentence_candidates = []\n",
        "  word =lemma(word)[0]\n",
        "  for candidate in part_of_speech_candidate(word):\n",
        "    new_sentence = re.sub(r'\\b' + word + r'\\b' , candidate ,sentence,count = 1)  \n",
        "    sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "  rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "  best_candidate = rank[0][0]\n",
        "  return best_candidate"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkSA6AqCLFqb",
        "colab_type": "text"
      },
      "source": [
        "# **Edit sentence Version1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8lAqzbpxZgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edit_sentence1(sentence):\n",
        "    threshold = 0   #จำนวนครั้งที่แก้\n",
        "    sentence_score = evaluate(model,sentence)\n",
        "    pos_sentence = pos_tag(sentence)\n",
        "    best_candidate = sentence\n",
        "\n",
        "    #article\n",
        "    for word,pos in pos_sentence:\n",
        "        if word in article_candidate():\n",
        "          best_candidate = edit_article(word,sentence)\n",
        "          threshold += 1\n",
        "\n",
        "    #be_verb\n",
        "    for word,pos in pos_sentence:\n",
        "      if word in be_verb():\n",
        "        best_candidate = edit_be_verb(word,best_candidate)\n",
        "        threshold += 1\n",
        "\n",
        "    #verb forms\n",
        "    for word,pos in pos_sentence:\n",
        "        if pos.startswith('V'):\n",
        "          best_candidate = edit_verb_forms(word,best_candidate)\n",
        "          threshold += 1\n",
        "\n",
        "    #noun\n",
        "    for word,pos in pos_sentence:\n",
        "        if pos.startswith('N'):\n",
        "          best_candidate = edit_noun(word,best_candidate)\n",
        "          threshold += 1\n",
        "\n",
        "    return best_candidate"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf_4TYTnQDCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "23f72b4e-5d44-462b-c2d0-aa678f6b7f28"
      },
      "source": [
        "edit_sentence1('I play the ball with cat')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I played  ball with cats'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFfttlPieFA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "14861253-9d0c-4650-9e75-e977c617e61d"
      },
      "source": [
        "edit_sentence1(sentences[100])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Genetic testing was made possible and available for one 's decision to undergo .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "139LOV4c_MMn",
        "colab_type": "text"
      },
      "source": [
        "# **Edit sentence Version 2**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoliDC5SS75r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ouput : corrected_file_1\n",
        "#data , tokens : nyt_eng_200001 /  9246188 tokens , 5 grams\n",
        "\n",
        "def edit_sentence2(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  urls = re.findall(r\"https?\", sentence)\n",
        "  if len(urls) > 0:\n",
        "    return sentence\n",
        "  else:\n",
        "  #Noun\n",
        "   for i in range(len(doc)):\n",
        "    tokens = [token.text for token in nlp(sentence)]\n",
        "    if doc[i].pos_ == \"NOUN\":\n",
        "      sentence_candidates = []\n",
        "      for candidate in noun_candidate(doc[i].lemma_):\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "    #Article\n",
        "    elif doc[i].text in article_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in article_candidate():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "    #Be_verb\n",
        "    elif doc[i].text in be_verb():\n",
        "      sentence_candidates = []\n",
        "      for candidate in be_verb():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "    #Verb_forms\n",
        "    elif doc[i].pos_ == \"VERB\":\n",
        "      sentence_candidates = []\n",
        "      for candidate in verb_form_candidate(doc[i].lemma_):\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "    \n",
        "    #Preposition\n",
        "    elif doc[i].pos_ == \"ADP\":\n",
        "      sentence_candidates = []\n",
        "      for candidate in prep_candidate():\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "  return sentence.capitalize() + '.'"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dASGhHLh7FDI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "791c9e3f-4102-4811-99ae-5c9774eb8b34"
      },
      "source": [
        "verb_dict2[\"be\"]"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['be', 'is', 'was', 'been', 'being']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnhl6vCCqZEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d63761a2-0972-465f-8561-4f542226db72"
      },
      "source": [
        "sentences[100]"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Genetic testing is made possible and available for one 's decision to undergo .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i1V9282JlNJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "465d5967-b06b-48ba-c8c5-941b0d982437"
      },
      "source": [
        "edit_sentence2(sentences[100])"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Genetic testing was made possible and available for one 's decision to undergo ..\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mniSSB6YzJuw",
        "colab_type": "text"
      },
      "source": [
        "# **Edit sentence version 3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rjt-SfJzJVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ouput : corrected_file_2\n",
        "#data , tokens : Gutenberg , 210907708 tokens\n",
        "\n",
        "def edit_sentence3(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  urls = re.findall(r\"https?\", sentence)\n",
        "  if len(urls) > 0:\n",
        "    return sentence\n",
        "  else:\n",
        "  #Noun\n",
        "   for i in range(len(doc)):\n",
        "    tokens = [token.text for token in nlp(sentence)]\n",
        "    if doc[i].pos_ == \"NOUN\":\n",
        "      sentence_candidates = []\n",
        "      for candidate in noun_candidate(doc[i].lemma_):\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "    #Article\n",
        "    elif doc[i].text in article_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in article_candidate():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "    #Be_verb\n",
        "    elif doc[i].text in be_verb():\n",
        "      sentence_candidates = []\n",
        "      for candidate in be_verb():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "    #Verb_forms\n",
        "    elif doc[i].pos_ == \"VERB\":\n",
        "      sentence_candidates = []\n",
        "      for candidate in verb_form_candidate(doc[i].lemma_):\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "      \n",
        "    #Preposition\n",
        "    elif doc[i].pos_ == \"ADP\":\n",
        "      sentence_candidates = []\n",
        "      for candidate in prep_candidate():\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((new_sentence,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      sentence = best_candidate\n",
        "\n",
        "  return sentence.capitalize() + '.'"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvt26ZhD6wJP",
        "colab_type": "text"
      },
      "source": [
        "# **Edit sentence Version4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUiNXC4VXshR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edit_sentence4(sentence):\n",
        "  pos_sentence = pos_tag(sentence)\n",
        "  urls = re.findall(r\"https?\", sentence)\n",
        "  if len(urls) > 0:\n",
        "    return sentence\n",
        "  else:\n",
        "  #Noun\n",
        "   tokens = [token for token,pos in pos_sentence]\n",
        "   for i in range(len(pos_sentence)):\n",
        "    \n",
        "    if pos_sentence[i][1].startswith(\"N\"):\n",
        "      sentence_candidates = []\n",
        "      default = pos_sentence[i][0]\n",
        "      stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      for candidate in noun_candidate(stem):\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "\n",
        "      if default[0].isupper() == True:\n",
        "        best_candidate = best_candidate.capitalize()\n",
        "        tokens[i] = best_candidate\n",
        "      else:\n",
        "        tokens[i] = best_candidate\n",
        "\n",
        "    #Article\n",
        "    elif pos_sentence[i][0] in article_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in article_candidate():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate\n",
        "\n",
        "    #Be_verb\n",
        "    elif pos_sentence[i][0] in be_verb():\n",
        "      sentence_candidates = []\n",
        "      for candidate in be_verb():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate\n",
        "\n",
        "    #Verb_forms\n",
        "    elif pos_sentence[i][1].startswith(\"V\"):\n",
        "      sentence_candidates = []\n",
        "      default = pos_sentence[i][0]\n",
        "      stem = nltk.stem.WordNetLemmatizer().lemmatize(pos_sentence[i][0], 'v')\n",
        "\n",
        "      for candidate in verb_form_candidate(stem):\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "\n",
        "      if i < len(pos_sentence)-1 and pos_sentence[i+1][0] not in prep_candidate():\n",
        "        sentence_candidates = []\n",
        "        for candidate in prep_candidate():\n",
        "          new_tokens = tokens[:]\n",
        "          if candidate == \"\" :\n",
        "             new_tokens[i] = best_candidate\n",
        "          else:\n",
        "            new_tokens[i] = best_candidate + \" \" + candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((new_tokens[i],evaluate(model,new_sentence)))\n",
        "        rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "        best_candidate = rank[0][0]\n",
        "\n",
        "      if default[0].isupper() == True:\n",
        "        best_candidate = best_candidate.capitalize()\n",
        "        tokens[i] = best_candidate\n",
        "      else:\n",
        "        tokens[i] = best_candidate\n",
        "    elif pos_sentence[i][1].startswith(\"J\"):\n",
        "      #stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      sentence_candidates = []\n",
        "      stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      if stem in verb_form_candidate(stem):\n",
        "        for vj_can in verb_form_candidate(stem):        \n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = vj_can\n",
        "          sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((vj_can,evaluate(model,sentence)))\n",
        "        rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "        best_candidate = rank[0][0]\n",
        "      else:\n",
        "        best_candidate = pos_sentence[i][0]\n",
        "     \n",
        "    #Preposition\n",
        "    elif pos_sentence[i][0] in prep_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in prep_candidate():\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate \n",
        "  return join_tokens(tokens)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NcSJCk88d_l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "71e88849-2922-43e4-aa64-69db803babee"
      },
      "source": [
        "edit_sentence4('I can eat')"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I can eat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKoiKC_TAZBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "60d16aeb-5c48-467a-9ee7-b8fd4481060c"
      },
      "source": [
        "edit_sentence4(sentences[1114])"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'However , it is good for sharing of information .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWDDaPSd0jQb",
        "colab_type": "text"
      },
      "source": [
        "# **Edit sentence Version5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1akg_Xdn6s4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edit_sentence5(sentence):\n",
        "  #doc = nlp(sentence)\n",
        "  pos_sentence = pos_tag(sentence)\n",
        "  urls = re.findall(r\"https?\", sentence)\n",
        "  if len(urls) > 0:\n",
        "    return sentence\n",
        "  else:\n",
        "  #Noun\n",
        "   tokens = [token for token,pos in pos_sentence]\n",
        "   for i in range(len(pos_sentence)):\n",
        "    \n",
        "    if pos_sentence[i][1].startswith(\"N\"):\n",
        "      sentence_candidates = []\n",
        "      default = pos_sentence[i][0]\n",
        "      stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      for candidate in noun_candidate(stem):\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "     # print(best_candidate)\n",
        "      #sentence = best_candidate\n",
        "      if default[0].isupper() == True:\n",
        "        best_candidate = best_candidate.capitalize()\n",
        "        tokens[i] = best_candidate\n",
        "      else:\n",
        "        tokens[i] = best_candidate\n",
        "\n",
        "    #Article\n",
        "    elif pos_sentence[i][0] in article_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in article_candidate():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "     # print(best_candidate)\n",
        "      #sentence = best_candidate\n",
        "      tokens[i] = best_candidate\n",
        "\n",
        "    #Be_verb\n",
        "    elif pos_sentence[i][0] in be_verb():\n",
        "      sentence_candidates = []\n",
        "      for candidate in be_verb():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "    #  print(best_candidate)\n",
        "      #sentence = best_candidate\n",
        "      tokens[i] = best_candidate\n",
        "\n",
        "    #Verb_forms\n",
        "    elif pos_sentence[i][1].startswith(\"V\"):\n",
        "      sentence_candidates = []\n",
        "      default = pos_sentence[i][0]\n",
        "      stem = nltk.stem.WordNetLemmatizer().lemmatize(pos_sentence[i][0], 'v')\n",
        "\n",
        "      for candidate in verb_form_candidate(stem):\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      if default[0].isupper() == True:\n",
        "        best_candidate = best_candidate.capitalize()\n",
        "        tokens[i] = best_candidate\n",
        "      else:\n",
        "        tokens[i] = best_candidate\n",
        "    elif pos_sentence[i][1].startswith(\"J\"):\n",
        "      #stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      sentence_candidates = []\n",
        "      stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      if stem in verb_form_candidate(stem):\n",
        "        for vj_can in verb_form_candidate(stem):        \n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = vj_can\n",
        "          sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((vj_can,evaluate(model,sentence)))\n",
        "        rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "        best_candidate = rank[0][0]\n",
        "      else:\n",
        "        best_candidate = pos_sentence[i][0]\n",
        "     # print(best_candidate)\n",
        "      #sentence = best_candidate\n",
        "    #Preposition\n",
        "    elif pos_sentence[i][0] in prep_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in prep_candidate():\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "     # print(best_candidate)\n",
        "      #sentence = best_candidate\n",
        "      tokens[i] = best_candidate \n",
        "  return join_tokens(tokens)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrQ881vKS5eu",
        "colab_type": "text"
      },
      "source": [
        "# Edit sentence Version5 with auto correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKIqwUtHJ9Yr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "99e36286-ebe8-4968-f16d-f2be385b50fb"
      },
      "source": [
        "!pip install autocorrect"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/5b/6510d8370201fc96cbb773232c2362079389ed3285b0b1c6a297ef6eadc0/autocorrect-2.0.0.tar.gz (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 2.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.0.0-cp36-none-any.whl size=1811641 sha256=35f0d69f24d216db4c9dca53917e45e7b1706abe9e96e9dcc5e935d295a233d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/06/bc/e66f28d72bed29591eadc79cebb2e7964ad0282804ab233da3\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE7zRwHRWksT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from autocorrect import Speller\n",
        "\n",
        "spell = Speller(lang='en')"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYfTQpSCTHc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edit_sentence5autocorrect(sentence):\n",
        "  sentence = spell(sentence)\n",
        "  pos_sentence = pos_tag(sentence)\n",
        "  urls = re.findall(r\"https?\", sentence)\n",
        "  if len(urls) > 0:\n",
        "    return sentence\n",
        "  else:\n",
        "  #Noun\n",
        "   tokens = [token for token,pos in pos_sentence]\n",
        "   for i in range(len(pos_sentence)):\n",
        "    \n",
        "    if pos_sentence[i][1].startswith(\"N\"):\n",
        "      sentence_candidates = []\n",
        "      default = pos_sentence[i][0]\n",
        "      stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      for candidate in noun_candidate(stem):\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "\n",
        "      if default[0].isupper() == True:\n",
        "        best_candidate = best_candidate.capitalize()\n",
        "        tokens[i] = best_candidate\n",
        "      else:\n",
        "        tokens[i] = best_candidate\n",
        "\n",
        "    #Article\n",
        "    elif pos_sentence[i][0] in article_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in article_candidate():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate\n",
        "\n",
        "    #Be_verb\n",
        "    elif pos_sentence[i][0] in be_verb():\n",
        "      sentence_candidates = []\n",
        "      for candidate in be_verb():\n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = candidate\n",
        "          new_sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate\n",
        "\n",
        "    #Verb_forms\n",
        "    elif pos_sentence[i][1].startswith(\"V\"):\n",
        "      sentence_candidates = []\n",
        "      default = pos_sentence[i][0]\n",
        "      stem = nltk.stem.WordNetLemmatizer().lemmatize(pos_sentence[i][0], 'v')\n",
        "\n",
        "      for candidate in verb_form_candidate(stem):\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      if default[0].isupper() == True:\n",
        "        best_candidate = best_candidate.capitalize()\n",
        "        tokens[i] = best_candidate\n",
        "      else:\n",
        "        tokens[i] = best_candidate\n",
        "    elif pos_sentence[i][1].startswith(\"J\"):\n",
        "      sentence_candidates = []\n",
        "      stem = lemmatizer.lemmatize(pos_sentence[i][0].lower())\n",
        "      if stem in verb_form_candidate(stem):\n",
        "        for vj_can in verb_form_candidate(stem):        \n",
        "          new_tokens = tokens[:]\n",
        "          new_tokens[i] = vj_can\n",
        "          sentence = \" \".join(new_tokens)\n",
        "          sentence_candidates.append((vj_can,evaluate(model,sentence)))\n",
        "        rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "        best_candidate = rank[0][0]\n",
        "      else:\n",
        "        best_candidate = pos_sentence[i][0]\n",
        "   \n",
        "    #Preposition\n",
        "    elif pos_sentence[i][0] in prep_candidate():\n",
        "      sentence_candidates = []\n",
        "      for candidate in prep_candidate():\n",
        "        new_tokens = tokens[:]\n",
        "        new_tokens[i] = candidate\n",
        "        new_sentence = \" \".join(new_tokens)\n",
        "        sentence_candidates.append((candidate,evaluate(model,new_sentence)))\n",
        "      rank = sorted(sentence_candidates , key = lambda x:x[1],reverse=True)\n",
        "      best_candidate = rank[0][0]\n",
        "      tokens[i] = best_candidate \n",
        "  return join_tokens(tokens)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMIDBqccYv0H",
        "colab_type": "text"
      },
      "source": [
        "#Generate File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1zN1IbMsVm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "971174884b444e8cb5b5fd3af54bc1fa",
            "27d35393d9c2449c9355dc8991b7eaf2",
            "61d2ba04df39455499d0c68effbc690a",
            "4a2a34e2296a4ef19c28087142cd4e9a",
            "aa4c54f9016f40fda141bc88ca736fa5",
            "ae7e7713ae1f456d9af47d6edbcba30c",
            "2c3cbfd6944b423d9c4c54c7a8ff6f73",
            "6289b6465bd44f428f51ff2b4784a162"
          ]
        },
        "outputId": "cd3bf89a-8937-44d5-9179-5fa1c3f8a4ed"
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "with open('corrected_sentences.txt', 'w') as f :\n",
        "  for line in tqdm_notebook(sentences):\n",
        "    f.write(edit_sentence5(line) + '\\n')"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "971174884b444e8cb5b5fd3af54bc1fa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfhZC-iNL1-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0589f811-b8ee-404b-ee69-849068d35bbd"
      },
      "source": [
        "!wc -l /content/kenlm/build/corrected_sentences.txt"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1313 /content/kenlm/build/corrected_sentences.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9Nqt52rNMmc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53dcff73-d7d1-4791-96f0-8638ce233183"
      },
      "source": [
        "!wc -l /content/kenlm/build/gec-test-set.txt"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1312 /content/kenlm/build/gec-test-set.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGrZOKza462k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "with open('corrected_file_5_newst.txt', 'w') as f :\n",
        "  for line in tqdm(sentences):\n",
        "    f.write(edit_sentence6(line) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wAxKS3lTXAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "with open('corrected_file_5_w_autocorrect.txt', 'w') as f :\n",
        "  for line in tqdm(sentences):\n",
        "    f.write(edit_sentence5autocorrect(line) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}